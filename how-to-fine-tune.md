N·∫øu b·∫°n ch·ªâ mu·ªën **fine-tune m√¥ h√¨nh** m√† kh√¥ng c·∫ßn training t·ª´ ƒë·∫ßu, th√¨ r·∫•t ƒë∆°n gi·∫£n! Project n√†y ƒë√£ cung c·∫•p c√°c m√¥ h√¨nh pre-trained s·∫µn.

Read file: LLaMA-Factory/data/gsm_test.json

Perfect! ƒê·ªÉ **ch·ªâ fine-tune m√¥ h√¨nh** m√† kh√¥ng c·∫ßn training t·ª´ ƒë·∫ßu, ƒë√¢y l√† h∆∞·ªõng d·∫´n chi ti·∫øt:

## üöÄ **C√°ch Fine-tune DiffuGPT/DiffuLLaMA**

### **1. M√¥ h√¨nh Pre-trained c√≥ s·∫µn**
- **DiffuGPT**: `diffusionfamily/diffugpt-s`, `diffusionfamily/diffugpt-m`
- **DiffuLLaMA**: `diffusionfamily/diffullama`


### **2. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng**
```bash
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
pip install flash-attn==2.6.3 --no-build-isolation
```

### **3. Chu·∫©n b·ªã d·ªØ li·ªáu**
D·ªØ li·ªáu c·∫ßn format JSON nh∆∞ sau:
```json
[
    {
        "instruction": "Gi·∫£i b√†i to√°n n√†y: 2+2 b·∫±ng bao nhi√™u?",
        "output": "2+2 = 4"
    },
    {
        "instruction": "C√¢u h·ªèi kh√°c...",
        "output": "C√¢u tr·∫£ l·ªùi..."
    }
]
```

**Th√™m dataset v√†o** `LLaMA-Factory/data/dataset_info.json`:
```json
"my_dataset": {
    "file_name": "my_data.json",
    "columns": {
        "prompt": "instruction",
        "response": "output"
    }
}
```

### **4. Fine-tune DiffuLLaMA v·ªõi LoRA**
**Config file: `my_diffullama_finetune.yaml`**
```yaml
### model
model_name_or_path: diffusionfamily/diffullama
print_param_status: true

### method
stage: ddm-sft
do_train: true
finetuning_type: lora
lora_target: all
additional_target: embed_tokens
lora_rank: 16

### dataset
dataset: my_dataset
template: empty
cutoff_len: 256
streaming: false
overwrite_cache: true

### output
output_dir: output/my-diffullama-finetuned/
logging_steps: 10
save_steps: 2000

### train
per_device_train_batch_size: 8
learning_rate: 1.0e-4
num_train_epochs: 5.0
bf16: true
anneal_steps: 1
shift: true
```

**Ch·∫°y fine-tuning:**
```bash
cd LLaMA-Factory
llamafactory-cli train my_diffullama_finetune.yaml
```

### **5. Fine-tune DiffuGPT**
**Config file: `my_diffugpt_finetune.yaml`**
```yaml
### model
model_name_or_path: diffusionfamily/diffugpt-m
checkpoint_dir: # ƒë·ªÉ tr·ªëng n·∫øu d√πng HuggingFace model

### method
stage: ddm-sft
do_train: true
finetuning_type: full

### dataset
dataset: my_dataset
template: empty
cutoff_len: 256

### output
output_dir: output/my-diffugpt-finetuned/

### train
per_device_train_batch_size: 64
learning_rate: 3.0e-4
num_train_epochs: 10.0
bf16: true
anneal_steps: 1
shift: true
```

### **6. Inference (Test m√¥ h√¨nh)**
**Config file: `my_inference.yaml`**
```yaml
### model
model_name_or_path: diffusionfamily/diffullama
adapter_name_or_path: output/my-diffullama-finetuned/checkpoint-2000

### method
stage: ddm-sft
do_predict: true
finetuning_type: lora
shift: true

### dataset
dataset: my_test_dataset
template: empty
cutoff_len: 256

### output
output_dir: output/predictions/

### eval
per_device_eval_batch_size: 32
logits_temp: 1.0
diffusion_steps: 64
```

**Ch·∫°y inference:**
```bash
llamafactory-cli train my_inference.yaml
```

### **7. S·ª≠ d·ª•ng Quick Inference**
```bash
# DiffuGPT
python inf_diffugpt.py --model_name output/my-diffugpt-finetuned/ --base_model_name gpt2-medium

# DiffuLLaMA
python inf_diffullama.py --model_name output/my-diffullama-finetuned/ --flash_attn flash_attention_2
```

## üéØ **C√°c ƒëi·ªÉm quan tr·ªçng:**

1. **LoRA vs Full Fine-tuning**: LoRA nhanh h∆°n, √≠t VRAM h∆°n, full fine-tuning hi·ªáu qu·∫£ h∆°n
2. **Dataset**: ƒê√£ c√≥ s·∫µn GSM8K dataset, b·∫°n c√≥ th·ªÉ test v·ªõi n√≥
3. **Parameters quan tr·ªçng**: 
   - `stage: ddm-sft` (diffusion supervised fine-tuning)
   - `shift: true` (b·∫Øt bu·ªôc cho diffusion models)
   - `anneal_steps: 1` (cho fine-tuning)

4. **Monitoring**: Theo d√µi loss trong qu√° tr√¨nh training
5. **Inference**: ƒêi·ªÅu ch·ªânh `diffusion_steps` v√† `logits_temp` ƒë·ªÉ ki·ªÉm so√°t ch·∫•t l∆∞·ª£ng generation

B·∫°n c√≥ mu·ªën t√¥i gi√∫p t·∫°o config c·ª• th·ªÉ cho dataset c·ªßa b·∫°n kh√¥ng?