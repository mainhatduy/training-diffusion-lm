### model
model_name_or_path: Dream-org/Dream-v0-Instruct-7B
hf_hub_token: 
print_param_status: true

### method
stage: ddm-sft
do_train: true
finetuning_type: lora
lora_target: all
additional_target: embed_tokens
lora_rank: 16

### dataset
dataset: my_dataset
template: empty
cutoff_len: 256
streaming: false
overwrite_cache: true

### output
output_dir: output/my-diffullama-finetuned-v3/
logging_steps: 10
save_total_limit: 1
save_steps: 100
# push_to_hub: true
# hub_model_id: myduy/test-diffusionlm-v2

### train
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 5e-6
warmup_ratio: 0.1
lr_scheduler_type: "linear"
num_train_epochs: 3.0
bf16: true
anneal_steps: 1
shift: true

### wandb logging
report_to: wandb
run_name: finetune-dream-7b-v1


### optional: push to huggingface hub after training
export_hub_model_id: "myduy/test-diffusionlm-v3"