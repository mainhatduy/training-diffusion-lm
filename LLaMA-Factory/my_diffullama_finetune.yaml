### model
model_name_or_path: diffusionfamily/diffullama
hf_hub_token: 
print_param_status: true

### method
stage: ddm-sft
do_train: true
finetuning_type: lora
lora_target: all
additional_target: embed_tokens
lora_rank: 16

### dataset
dataset: my_dataset
template: empty
cutoff_len: 256
streaming: false
overwrite_cache: true

### output
output_dir: output/my-diffullama-finetuned-v3/
logging_steps: 10
save_total_limit: 1
save_steps: 100
# push_to_hub: true
# hub_model_id: myduy/test-diffusionlm-v2

### train
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 1.0e-4
num_train_epochs: 1.0
bf16: true
anneal_steps: 1
shift: true

### wandb logging
report_to: wandb
run_name: finetune-diffusion-lm-v1


### optional: push to huggingface hub after training
export_hub_model_id: "myduy/test-diffusionlm-v2"